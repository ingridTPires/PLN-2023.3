{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingridTPires/PLN-2023.3/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/11 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` David Oliveira Silva - 11201721357\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Ingrid Teixeira Pires - 11201812175\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Bruno Delphini Fontanezi - 11202130336"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: ` 5\n",
        "\n",
        "`Segundo capítulo:` 21\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHzDNMMaS1bn",
        "outputId": "417f9a66-ed08-4a04-d0fa-24a263b9d61f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import time\n",
        "\n",
        "openai.api_key = 'sua_chave_aqui'\n",
        "endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "# URL do primeiro capítulo\n",
        "url_capitulo_5 = \"https://brasileiraspln.com/livro-pln/1a-edicao/parte3/cap5/cap5.html\"\n",
        "# URL do segundo capítulo\n",
        "url_capitulo_21 = \"https://brasileiraspln.com/livro-pln/1a-edicao/parte9/cap21/cap21.html\""
      ],
      "metadata": {
        "id": "ri1zz3PtqX-H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extrair_texto_de_todas_as_secoes(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontre todas as tags  e extraia o texto de cada uma delas\n",
        "    sections = soup.find_all('section')\n",
        "    texts = [section.get_text() for section in sections]\n",
        "\n",
        "    # Junte todos os textos em uma única string\n",
        "    combined_text = '\\n'.join(texts)\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "def correcao_gramatical(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá corrigir gramaticalmente o seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario},]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def traducao_texto(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá traduzir para o ingles o seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario},]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def palavras_chave(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá extrair todas as palavras chave do seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario},]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def sumarizar_texto(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá sumarizar o seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario},]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def analise_sentimentos(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá analisar sentimentos do seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "    mensagem_assistente = \"Classifique o sentimento retornando apenas 'Positivo' ou 'Negativo'. \"\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario},\n",
        "                {\"role\": \"assistant\", \"content\": mensagem_assistente}]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def classificacao_textos(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá classificar o seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario}]}\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\"\n",
        "\n",
        "def deteccao_emocoes(trecho_livro):\n",
        "    mensagem_sistema = 'Você é um assistente que irá detectar as emoções do seguinte texto'\n",
        "    mensagem_usuario = trecho_livro\n",
        "\n",
        "    parametros = {\n",
        "    \"model\": \"gpt-3.5-turbo-0613\",\n",
        "    \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": mensagem_sistema},\n",
        "                {\"role\": \"user\", \"content\": mensagem_usuario}]},\n",
        "    headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "\n",
        "    resposta = requests.post(endpoint, json=parametros, headers=headers)\n",
        "\n",
        "    if resposta.status_code == 200:\n",
        "        return resposta.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"Falha na solicitação da API.\""
      ],
      "metadata": {
        "id": "CY5qWBFtm0u-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aplicar_tecnicas_pln(trecho_livro):\n",
        "  print(\"*Correção gramatical: \"+correcao_gramatical(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(10) #tempo apenas para nao passar do limite gratuito de 3 consultas por minuto\n",
        "  print(\"*Tradução do texto: \"+traducao_texto(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(10)\n",
        "  print(\"*Extração de Palavras-chave: \"+palavras_chave(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(10)\n",
        "  print(\"*Sumarização de Texto: \"+sumarizar_texto(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(10)\n",
        "  print(\"*Análise de sentimentos: \"+analise_sentimentos(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(10)\n",
        "  print(\"*Classificação de texto: \"+classificacao_textos(trecho_livro))\n",
        "  print('\\n')\n",
        "\n",
        "  time.sleep(20)\n",
        "  print(\"*Detecção de emoções: \"+deteccao_emocoes(trecho_livro))\n",
        "  print('\\n')\n",
        "  time.sleep(10)"
      ],
      "metadata": {
        "id": "xpdkrzF8n0FH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_capitulo_5 = extrair_texto_de_todas_as_secoes(url_capitulo_5)\n",
        "texto_capitulo_21 = extrair_texto_de_todas_as_secoes(url_capitulo_21)"
      ],
      "metadata": {
        "id": "VBAjFl3L1QDU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CAPÍTULO 5:\")\n",
        "aplicar_tecnicas_pln(texto_capitulo_5)\n",
        "\n",
        "print(\"CAPÍTULO 21:\")\n",
        "aplicar_tecnicas_pln(texto_capitulo_21)"
      ],
      "metadata": {
        "id": "Upm75zF8nm1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44559162-354c-4cb7-ca66-28d4c70b29ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAPÍTULO 5:\n",
            "*Correção gramatical: 5.1 Cenas dos próximos capítulos...\n",
            "O capítulo sobre as expressões multi palavras, que sairá na próxima edição deste livro, vai abordar um tema indigesto no universo do Processamento de Linguagem Natural (PLN). É que essas expressões ficam no limite entre a sintaxe e a semântica e sempre acabam ficando no meio do fogo cruzado. De um lado, elas apresentam idiossincrasias e especificidades que não permitem determinadas operações sintáticas comuns a outros conjuntos de palavras. De outro, as definições do sentido das expressões multi palavras (ou MWEs) em geral fogem à regra de que o significado do todo se dá pela soma das partes. Ainda bem, pois imagine se a expressão \"engolir o sapo\" fosse literal.\n",
            "Assim, a nossa intenção para este capítulo é, em primeiro lugar, definir quais são os conceitos fundamentais para quem vai navegar pelas águas turbulentas do tratamento computacional de MWEs. Entre esses conceitos, começaremos com a discussão sobre os elementos que compõem uma MWE: seriam palavras? Seriam lexemas?\n",
            "Na sequência, abordaremos os conceitos de MWEs propriamente ditos. A literatura traz inúmeras maneiras de analisar essas expressões e as definições são tão variadas quanto os campos de estudo que se interessam por esse tema. Em PLN, escolhemos a definição utilizada pelo projeto PARSEME (que também é apresentado brevemente no capítulo que está por vir):\n",
            "\n",
            "Expressões multi palavras são entendidas como sequências (contínuas ou descontínuas) de palavras que (i) contêm pelo menos duas palavras componentes que são lexicalizadas, ou seja, sempre realizadas pelos mesmos lexemas, incluindo uma palavra principal e pelo menos uma outra palavra sintaticamente relacionada, e (ii) exibem algum grau de idiossincrasia lexical, morfológica, sintática e/ou semântica.\n",
            "\n",
            "Considerando essa definição inicial de MWE, vamos pormenorizar alguns dos seus aspectos importantes, como idiossincrasias, coesão sintática e elementos lexicalizados. Ao mesmo tempo, também buscamos separar o joio do trigo, uma vez que, para entender o que é uma MWE, é fundamental que se saiba também o que ela não é. Então, explicaremos por que, na nossa abordagem, os compostos, as colocações e as metáforas não são expressões multi palavras.\n",
            "Embora esta seja uma tarefa em aberto, apresentaremos, no nosso futuro capítulo, uma tentativa de classificação de MWEs. O grupo de expressões cujo núcleo é um verbo foi definido nas diretrizes de anotação do PARSEME, e nós basicamente utilizamos as definições desse projeto. No que se refere às demais categorias, consideramos uma série de aspectos práticos e teóricos na nossa proposta de categorização, levando em conta sobretudo a sua função em uma sentença. Traremos, ainda, algumas discussões sobre ambiguidade, variabilidade e arbitrariedade.\n",
            "Após a etapa de conceituações, citaremos as principais tarefas de PLN que envolvem MWEs, assim como o estado da arte, os métodos e algoritmos que buscam resolver esse problema. Essas tarefas se dividem basicamente em dois grandes grupos: a) a descoberta/detecção; e b) a identificação de MWEs.\n",
            "Você verá no capítulo, ainda, uma breve apresentação dos recursos existentes para o português brasileiro, assim como o conhecimento necessário para entender ou resolver alguma tarefa em torno da questão das MWEs. Depois, trataremos dos principais métodos de avaliação, assim como métricas e testes comumente utilizados pela comunidade para avaliar o desempenho de métodos e algoritmos direcionados a tratar ou resolver as tarefas antes mencionadas.\n",
            "Por fim, consideramos importante olhar pelo retrovisor e entender qual foi o percurso para chegar até onde estamos, no tema das MWEs, em termos científicos, na direção da resolução dessa complexa tarefa. Em que ponto estamos hoje? Qual é a posição do português brasileiro em termos de recursos e de pesquisas diante da comunidade internacional? Quais são os desafios que permanecem aos pesquisadores e entusiastas das MWEs para descascar esse abacaxi em tempos de modelos de língua tão grandes que não cabem em si?\n",
            "Sem querer prometer mundos e fundos, o capítulo que está por vir tentará acrescentar mais um tijolo na construção do conhecimento linguístico em PLN para o português brasileiro. Como você pôde perceber neste resumo, o capítulo contém uma série de exemplos de expressões multi palavras, tanto para ilustrar fenômenos linguísticos quanto para divertir leitores e leitoras. Vale a pena esperar pelas cenas do nosso próximo capítulo!\n",
            "\n",
            "\n",
            "*Tradução do texto: 5.1 Scenes from the next chapters...\n",
            "The chapter on multiword expressions, which will be featured in the next edition of this book, will address a challenging topic in the field of Natural Language Processing (NLP). These expressions exist on the boundary between syntax and semantics, and always end up caught in the crossfire. On one hand, they have idiosyncrasies and specificities that prevent certain syntactic operations common to other word sets. On the other hand, the definitions of the meaning of multiword expressions (MWEs) generally deviate from the rule that the whole meaning is the sum of its parts. Fortunately, imagine if the expression \"engolir o sapo\" (literal: swallow the toad) were taken literally.\n",
            "Thus, our intention for this chapter is, first and foremost, to define the fundamental concepts for those navigating the turbulent waters of computational treatment of MWEs. Among these concepts, we will begin with a discussion on the elements that compose an MWE: are they words? Are they lexemes?\n",
            "Next, we will address the concepts of MWEs themselves. The literature presents numerous ways to analyze these expressions, and the definitions are as varied as the fields of study interested in this topic. In NLP, we chose the definition used by the PARSEME project (which is also briefly presented in the upcoming chapter):\n",
            "\n",
            "Multiword expressions are understood as (continuous or discontinuous) sequences of words that (i) contain at least two component words that are lexicalized, that is, always realized by the same lexemes, including a head word and at least one other word that is syntactically related, and (ii) exhibit some degree of lexical, morphological, syntactic, and/or semantic idiosyncrasy.\n",
            "\n",
            "Considering this initial definition of MWE, we will delve into some of its important aspects, such as idiosyncrasies, syntactic cohesion, and lexicalized elements. At the same time, we also aim to separate the wheat from the chaff, as it is crucial to understand what an MWE is by also knowing what it is not. Therefore, we will explain why, in our approach, compounds, collocations, and metaphors are not multiword expressions.\n",
            "Although this is an ongoing task, we will present, in our future chapter, an attempt at classifying MWEs. The group of expressions whose core is a verb was defined in the annotation guidelines of PARSEME, and we basically used the definitions from that project. Regarding the other categories, we considered a series of practical and theoretical aspects in our proposed categorization, especially taking into account their function in a sentence. We will also discuss ambiguity, variability, and arbitrariness.\n",
            "After the conceptual phase, we will mention the main NLP tasks involving MWEs, as well as the state of the art, methods, and algorithms aimed at solving this problem. These tasks can be broadly divided into two major groups: a) discovery/detection; and b) identification of MWEs.\n",
            "In the chapter, you will also find a brief overview of the existing resources for Brazilian Portuguese, as well as the necessary knowledge to understand or tackle any task related to MWEs. We will then discuss the main evaluation methods, as well as metrics and tests commonly used by the community to assess the performance of methods and algorithms aimed at dealing with or solving the aforementioned tasks.\n",
            "Finally, it is important to look back and understand the journey that has led us to where we are today, in terms of scientific advancements in the field of MWEs, with regards to the resources and research in Brazilian Portuguese in relation to the international community. What challenges remain for researchers and enthusiasts of MWEs in unraveling this complex task, especially in times of language models that are so large they almost defy comprehension?\n",
            "Without making lofty promises, the upcoming chapter will attempt to contribute another building block to the linguistic knowledge in NLP for Brazilian Portuguese. As you can see from this summary, the chapter includes a series of examples of multiword expressions, both to illustrate linguistic phenomena and to entertain readers. It's worth looking forward to the scenes from our next chapter!\n",
            "\n",
            "\n",
            "*Extração de Palavras-chave: palavras chave: expressões multipalavras, sintaxe, semântica, idiossincrasias, especificidades, operações sintáticas, definições, sentido, todo, partes, engolir o sapo, conceitos fundamentais, navegrar, tratamento computacional, MWEs, palavras, lexemas, sequências, lexicalizadas, componentes lexicalizadas, idiossincrasia lexical, morfológica, sintática, semântica, coesão sintática, elementos lexicalizados, compostos, colocações, metáforas, classificação de MWEs, grupo de expressões, núcleo, verbo, diretrizes de anotação, categorização, função em uma sentença, ambiguidade, variabilidade, arbitrariedade, tarefas de PLN, estado da arte, métodos, algoritmos, recursos existentes, português brasileiro, avaliação, métricas, testes, comunidade international, desafios, pesquisadores, entusiastas, modelos de língua, construção do conhecimento linguístico, fenômenos linguísticos, ilustrar, divertir, leitores, leitoras.\n",
            "\n",
            "\n",
            "*Sumarização de Texto: O próximo capítulo do livro abordará o tema das expressões multipalavras, que são expressões que ficam no limite entre a sintaxe e a semântica. Essas expressões apresentam idiossincrasias e especificidades que impedem determinadas operações sintáticas comuns a outros conjuntos de palavras. Além disso, as definições do sentido das expressões multipalavras fogem à regra de que o significado do todo se dá pela soma das partes. O capítulo buscará definir os conceitos fundamentais relacionados às expressões multipalavras, como os elementos que as compõem e as definições utilizadas. Serão discutidos também os conceitos de MWEs propriamente ditos, as idiossincrasias, coesão sintática e elementos lexicalizados. Será explicado por que compostos, colocações e metáforas não são considerados expressões multipalavras. O capítulo também apresentará uma tentativa de classificação de MWEs, abordará as principais tarefas de PLN envolvendo MWEs, os recursos existentes para o português brasileiro e os métodos de avaliação. Por fim, o capítulo fará uma reflexão sobre o estado atual das pesquisas em MWEs, os desafios futuros e a posição do português brasileiro nesse campo. O capítulo conterá exemplos de expressões multipalavras para ilustrar fenômenos linguísticos e entreter os leitores.\n",
            "\n",
            "\n",
            "*Análise de sentimentos: Positivo\n",
            "\n",
            "\n",
            "*Classificação de texto: O texto trata do tema das expressões multipalavras (MWEs) no Processamento de Linguagem Natural (PLN). O autor menciona que essas expressões estão no limite entre a sintaxe e a semântica e apresentam idiossincrasias e especificidades que as diferenciam de outros conjuntos de palavras. O objetivo do capítulo é definir conceitos fundamentais para o tratamento computacional de MWEs e discutir as várias definições e formas de análise dessas expressões. O autor também aborda a classificação de MWEs, as principais tarefas de PLN relacionadas a elas e os recursos existentes para o português brasileiro. Além disso, são mencionados métodos de avaliação e o estado da arte na área de MWEs. O texto conclui refletindo sobre os desafios e o papel do português brasileiro em relação à comunidade internacional.\n",
            "\n",
            "\n",
            "*Detecção de emoções: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "\\CAPÍTULO 21:\n",
            "*Correção gramatical: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Tradução do texto: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Extração de Palavras-chave: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Sumarização de Texto: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Análise de sentimentos: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Classificação de texto: Falha na solicitação da API.\n",
            "\n",
            "\n",
            "*Detecção de emoções: Falha na solicitação da API.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}